{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSpVBPe6W978"
      },
      "source": [
        "# XAI ASSIGNMENT 4\n",
        "# NAME: TIFFANY DEGBOTSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7Cd8ixTI-Xj",
        "outputId": "4a1e8718-9cd8-4240-9100-6733d1eb96e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.25.2\n",
            "  Downloading numpy-1.25.2.tar.gz (10.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Collecting alepython\n",
            "  Cloning https://github.com/MaximeJumelle/ALEPython.git (to revision dev) to /tmp/pip-install-h4fgxciq/alepython_b478fb7acb3843239efa21be2fbecd93\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MaximeJumelle/ALEPython.git /tmp/pip-install-h4fgxciq/alepython_b478fb7acb3843239efa21be2fbecd93\n",
            "  Resolved https://github.com/MaximeJumelle/ALEPython.git to commit 286350ab674980a32270db2a0b5ccca1380312a7\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting loguru>=0.4.1 (from alepython)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.12/dist-packages (from alepython) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.12/dist-packages (from alepython) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from alepython) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from alepython) (1.16.2)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from alepython) (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->alepython) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->alepython) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->alepython) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->alepython) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->alepython) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->alepython) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->alepython) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=2.2.3->alepython) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.22.0->alepython) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.22.0->alepython) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=2.2.3->alepython) (1.17.0)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: alepython\n",
            "  Building wheel for alepython (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alepython: filename=alepython-0.1.dev17+g286350ab6-py3-none-any.whl size=15251 sha256=597c286edc5e34ec9369561e3ae7eec515b820a218a36e4363568873dbf296c4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-78s8he9t/wheels/87/54/8b/9357059e0e9ade0ea889a30fe56beaed44312e07475133b2cc\n",
            "Successfully built alepython\n",
            "Installing collected packages: loguru, alepython\n",
            "Successfully installed alepython-0.1.dev17+g286350ab6 loguru-0.7.3\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.25.2 pandas==2.0.3 scikit-learn==1.2.2 shap==0.45.1\n",
        "!pip install git+https://github.com/MaximeJumelle/ALEPython.git@dev#egg=alepython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DUzwROK5JDaS"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Models\n",
        "import xgboost\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# XAI\n",
        "import shap\n",
        "from alepython import ale_plot\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaaVxMhCSXoS"
      },
      "source": [
        "# Using IMDB DATASET\n",
        "### The dataset I chose had a problematic row"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\n",
        "    \"src/TMDB_movie_dataset_v11.csv\",\n",
        "    engine=\"python\",        # more flexible parser\n",
        "    on_bad_lines=\"skip\"     # skip problematic rows\n",
        ")\n"
      ],
      "metadata": {
        "id": "YQLn4EAPiOuz",
        "outputId": "6ccdac04-54ce-4f7c-cabe-ea84f9b01558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'src/TMDB_movie_dataset_v11.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3539633593.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m data = pd.read_csv(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"src/TMDB_movie_dataset_v11.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python\"\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# more flexible parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'src/TMDB_movie_dataset_v11.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting the data"
      ],
      "metadata": {
        "id": "wrPyZO23ikTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "AbmTUz81ijg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dropping rows that have missing values in important columns(eye inspecting of features not use of feature importance)"
      ],
      "metadata": {
        "id": "J4ysEIf1kKFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values in important cols\n",
        "data = data.dropna(subset=[\"budget\", \"popularity\", \"runtime\", \"vote_average\", \"vote_count\"])"
      ],
      "metadata": {
        "id": "bU-zN31ekL7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The training was taking long so I decided to reduce my dataset\n",
        "# Sampled Data"
      ],
      "metadata": {
        "id": "ulO4Mj1ulujr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sampled = data.sample(5000, random_state=42)\n"
      ],
      "metadata": {
        "id": "ApFEnFWXly0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_sampled.head()"
      ],
      "metadata": {
        "id": "TypyPkQLmHeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining target and features\n",
        "## I chose just four features to help me interpret better since they are figures and would be easier to work with"
      ],
      "metadata": {
        "id": "WHY5UxnrkPF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_sampled[[\"budget\", \"popularity\", \"runtime\", \"vote_count\"]]\n",
        "y = data_sampled[\"vote_average\"]"
      ],
      "metadata": {
        "id": "EWlz4PjmkXBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "kIX2eLgels2v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx1XzZUIJTDA"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Train RandomForestRegressor model\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I included this beacause I noticed that after the PDP plots, the x axis wasn't showing the budget in millions of dollars. I read on it and found out PartialDependenceDisplay.from_estimator() internally uses Matplotlib to draw the figure and it has a default tick formatter."
      ],
      "metadata": {
        "id": "rIRlwBRx_T47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_sampled[\"budget\"].describe()\n"
      ],
      "metadata": {
        "id": "n8Q5o5zd-0UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9WMrsxrSc4Z"
      },
      "source": [
        "## Partial Dependence Plot for Budget\n",
        "### From the plot below, we see that a lower budget of around 0- 10 million gives a low rating arou 1.95 but as the budget increases between 20 million and around 100 million, the curve jumps up with the rating being around 2.25. Beyond around 150 million though, the curve flats out which means increasing budget further doesn't change predictions much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ongpmbktTE6W"
      },
      "source": [
        "#### Using PartialDependenceDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzIBxd1eJs_S"
      },
      "outputs": [],
      "source": [
        "# Choosing the feature of interest\n",
        "features = [\"budget\"]\n",
        "\n",
        "# Use PartialDependenceDisplay to plot PDP\n",
        "PartialDependenceDisplay.from_estimator(model, X_test, features, kind='average') #kind='both'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgioG7rlTIUT"
      },
      "source": [
        "## Build my own PDP with numpy\n",
        "## Based on this plot, budget does not meaningfully affect movie ratings. Predictions hover around 1.7 no matter if the budget is tiny or huge.\n",
        "## There is a spike at budgets near 0 which might be due to noise\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "513bNGTyE8Ev"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Choose the feature for which you want to plot partial dependence\n",
        "feature_index = 0  # 0 = budget, 1 = popularity, 2 = runtime, 3 = vote_count\n",
        "feature_name = X.columns[feature_index]\n",
        "\n",
        "# Create a grid of values for the chosen feature\n",
        "feature_values = np.linspace(\n",
        "    np.min(X.iloc[:, feature_index]),\n",
        "    np.max(X.iloc[:, feature_index]),\n",
        "    num=80\n",
        ")\n",
        "\n",
        "# Initialize array to store average predictions\n",
        "average_predictions = np.zeros_like(feature_values)\n",
        "\n",
        "# Duplicate the dataset to modify feature values\n",
        "X_modified = X.copy()\n",
        "\n",
        "# Loop over feature values\n",
        "for i, value in enumerate(feature_values):\n",
        "    # Replace chosen feature with the current grid value\n",
        "    X_modified.iloc[:, feature_index] = value\n",
        "\n",
        "    # Predict using the modified dataset\n",
        "    predictions = model.predict(X_modified)\n",
        "\n",
        "    # Store the average prediction\n",
        "    average_predictions[i] = np.mean(predictions)\n",
        "\n",
        "# Plot the partial dependence for the chosen feature\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(feature_values, average_predictions, linewidth=2)\n",
        "plt.xlabel(feature_name)\n",
        "plt.ylabel(\"Average predicted vote_average\")\n",
        "plt.title(f\"Partial Dependence for {feature_name}\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmxsJ2L0UITe"
      },
      "source": [
        "# Varying Grid Solution\n",
        "### The analysis reveals that the model has learned a negative relationship between budget and predicted ratings. In particular, most of the variation occurs within the lower-budget range, where even small increases in budget lead to a noticeable decline in predicted ratings. Beyond this range, the effect diminishes, and additional increases in budget have little to no impact as the curve levels off. Importantly, the consistency of this downward trend across multiple grid resolutions suggests that the relationship is stable and not an artifact of the sampling method. I found this suspicious so I decided to plot a correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wFguCx7PU8a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import to_rgba\n",
        "import numpy as np\n",
        "\n",
        "# Pick the feature index (0 = budget, 1 = popularity, 2 = runtime, 3 = vote_count)\n",
        "feature_index = 0\n",
        "feature_name = X.columns[feature_index]\n",
        "\n",
        "# Define different grid lengths (how many points we evaluate PDP on)\n",
        "grid_lengths = np.linspace(20, 120, 40).astype(int)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "# Define a base color and alpha (transparency) values\n",
        "base_color = (0.2, 0.4, 0.6)  # Blue-ish color\n",
        "min_alpha = 0.2\n",
        "max_alpha = 1.0\n",
        "\n",
        "# Normalize grid_lengths between 0 and 1 for transparency scaling\n",
        "normalized_g = (grid_lengths - grid_lengths.min()) / (grid_lengths.max() - grid_lengths.min())\n",
        "\n",
        "for i, g in enumerate(grid_lengths):\n",
        "    # Create feature grid with g points\n",
        "    feature_values = np.linspace(\n",
        "        np.min(X.iloc[:, feature_index]),\n",
        "        np.max(X.iloc[:, feature_index]),\n",
        "        num=g\n",
        "    )\n",
        "\n",
        "    # Initialize array to store average predictions\n",
        "    average_predictions = np.zeros_like(feature_values, dtype=float)\n",
        "\n",
        "    # Duplicate the dataset to modify feature values\n",
        "    X_modified = X.copy()\n",
        "\n",
        "    # Loop over feature values\n",
        "    for j, value in enumerate(feature_values):\n",
        "        X_modified.iloc[:, feature_index] = value\n",
        "        predictions = model.predict(X_modified)\n",
        "        average_predictions[j] = np.mean(predictions)\n",
        "\n",
        "    # Calculate color with alpha based on grid length\n",
        "    alpha = min_alpha + (max_alpha - min_alpha) * normalized_g[i]\n",
        "    color = to_rgba(base_color, alpha)\n",
        "\n",
        "    # Plot the partial dependence curve\n",
        "    ax.plot(feature_values, average_predictions, color=color)\n",
        "\n",
        "ax.set_xlabel(feature_name)\n",
        "ax.set_ylabel(\"Average predicted vote_average\")\n",
        "ax.set_title(f\"Partial Dependence for {feature_name}\\n(Varying Grid Resolution)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Matrix\n",
        "The correlation matrix shows that budget and vote_count have the strongest positive relationship (0.60), indicating that higher-budget films typically receive more votes. Other feature pairs, such as budget with runtime (0.25) and budget with popularity (0.22), show only weak correlations. Overall, aside from the overlap between budget and vote_count, the features are largely independent, suggesting minimal multicollinearity in the dataset."
      ],
      "metadata": {
        "id": "MCV1rmzZG3v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = X.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "92ukOSizG5q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ICE PLOT"
      ],
      "metadata": {
        "id": "mXJkFT7sE5Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing the feature of interest\n",
        "features = [\"budget\"]\n",
        "\n",
        "# Use PartialDependenceDisplay to plot PDP\n",
        "PartialDependenceDisplay.from_estimator(model, X_test, features, kind='individual')"
      ],
      "metadata": {
        "id": "WjZb1LrSE6rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4na5VBGOath"
      },
      "source": [
        "## ALE Plots üìâ\n",
        "\n",
        "Accumulated Local Effects (ALE) Plots [Paper, 2020](https://www.scholars.northwestern.edu/en/publications/visualizing-the-effects-of-predictor-variables-in-black-box-super)\n",
        "\n",
        "**How to create an ALE plot:**\n",
        "1. Bin the Feature: Divide the feature of interest into several intervals (bins). These bins help in managing the data and computing local effects within smaller, more manageable segments.\n",
        "2. Compute Local Effects: For each bin, calculate the local effect of the feature on the prediction. This involves: Calculating the change in prediction when moving from the lower to the upper edge of the bin\n",
        "Averaging this change over all instances that fall into that bin\n",
        "3. Accumulate Effects: Starting from the first bin, accumulate the local effects across all bins. Sum up the average effects sequentially to show how the feature influences the prediction as its value changes\n",
        "4. Centering: To make the plot more interpretable, center the accumulated effects around zero. Subtract the mean of the accumulated effects, which forces the interpretation to focus on deviations from the average prediction\n",
        "\n",
        "There are a few python implementations of ALE plots, here we show the [ALEPython implementation](https://github.com/blent-ai/ALEPython).\n",
        "\n",
        "The implementation is more complex and less intuitive than PDPs, with many hyperparameters, including:\n",
        "* **bins** - This parameter defines the number of bins to divide the range of the specified feature into. A larger number of bins can provide more granularity in the ALE plot but might also increase computation time.\n",
        "* **monte_carlo** - This parameter is a boolean flag indicating whether to use Monte Carlo sampling to estimate the ALE. Monte Carlo sampling can be beneficial when the number of samples in the dataset is large, as it reduces computational burden.\n",
        "* **monte_carlo_rep** - This parameter specifies the number of Monte Carlo replicates to use for estimating the ALE. More replicates can lead to a more accurate estimation but may also increase computation time.\n",
        "* **monte_carlo_ratio** - This parameter determines the proportion of the dataset to use for Monte Carlo sampling. It's a value between 0 and 1, where 1 means using the entire dataset. Using a smaller ratio can speed up computation but may introduce some sampling error.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAxyYF6VOLDr"
      },
      "source": [
        "This can take a couple of minutes to run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRnv6MJGNX3A"
      },
      "outputs": [],
      "source": [
        "# Use default parameters for 1D Main Effect ALE Plot\n",
        "ale_plot(model, X_train, 'budget', monte_carlo=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhfPbardQq__"
      },
      "outputs": [],
      "source": [
        "# Change hyperparameters for 1D Main Effect ALE Plot\n",
        "ale_plot(\n",
        "    model,\n",
        "    X_train,\n",
        "    \"budget\",\n",
        "    bins=5,\n",
        "    monte_carlo=True,\n",
        "    monte_carlo_rep=30,\n",
        "    monte_carlo_ratio=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWVgjNuaVAQ3"
      },
      "source": [
        "#### How to Interpret 1D Main Effect ALE Plot\n",
        "\n",
        "\n",
        "\n",
        "* X-axis represents feature values\n",
        "* Y-axis shows average effect on predictions\n",
        "* Each curve represents a feature's ALE. - Flat curves imply little impact; steep curves, significant impact\n",
        "* Upward curves: increasing feature value increases predictions; downward, the opposite\n",
        "* Steeper curves signify larger effects\n",
        "\n",
        "We can compare ALE plots to gauge relative feature importance.\n",
        "Features with steeper curves have larger impacts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3L7Abs6Q5z_"
      },
      "outputs": [],
      "source": [
        "# 2D Second-Order ALE Plot\n",
        "ale_plot(model, X_train, X_train.columns[:2], bins=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-WV8-fVkDS"
      },
      "source": [
        "#### How to Interpret - 2D Second-Order ALE Plot\n",
        "\n",
        "* Both axes represent the values of the two features being analyzed.\n",
        "* Each axis corresponds to one of the features.\n",
        "* The plot displays a surface where the height represents the average effect on predictions. Higher points indicate regions where the model tends to make higher predictions, and vice versa.\n",
        "* Patterns in the surface reveal how the joint behavior of the two features affects the model's predictions. Peaks or valleys suggest regions where the joint effect is particularly strong.\n",
        "* The direction of the slope indicates whether increasing one feature while holding the other constant tends to increase or decrease predictions. Steeper slopes represent larger effects, while flatter regions indicate smaller effects.\n",
        "\n",
        "\n",
        "We can compare the 2D Second-Order ALE Plot with individual ALE plots for each feature to understand how joint effects differ from marginal effects."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}